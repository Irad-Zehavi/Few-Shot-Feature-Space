{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot-Feature-Space\n",
    "\n",
    "> Demonstrating geometrical phenomena in feature spaces of few-shot learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Collapse (https://arxiv.org/abs/2008.08186) is a phenomenon where after enough traning, a model's feature space presents a specific geometrical structure, namely small intra-class variation, and class means that form a simplex ETF structure.\n",
    "Similarly, in this work I show demonstrate geometrical properties that arise in feature extractors used for few-shot learning (e.g. pair matching using Siamese networks), for classes unseen during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Specifically, we see that $d$-dimensional feature vectors of class $C$ distribute as a Gaussian $\\mathcal{N}(\\mu_C, \\Sigma_C)$, where the differnet class means $\\mu_C$ also distribute as a Gaussian. We also see that almost all of the variability in feature space is represented in a $k$-dimensional space for $k<d$ (e.g. for LFW $k \\approx d/10$)\n",
    "\n",
    "As natural distributions often tend to a Gaussian distribution, we can assume that \"ideal class features\" (prototypes) and the intra-class variations distribute as Gaussians. Therefore, these findings suggest that the learning process correctly identifies this set of features (and other redundant ones) as the best features to learn the large set of classes seen during training, which would naturally lead to good generalization to new classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sidebar you'll find differnet links to different pages, demonstrating the differnet geometrical properties:\n",
    "\n",
    "* `Normal Distribution` shows that features tend to distribute as Gaussians\n",
    "* `Class Mean PCA` shows that a low-dimensional subspace contains all of the feature information\n",
    "* `Inter-Class` shows that classes tend to be orthogonal to one another\n",
    "* `Intra-Class` shows that classes tend to be tightly concentrated (angle-wise). Together with the previous property, this explains the effectiveness of such feature extractors as backbones for Siamese Netowrks (since inter-class and intra-class angles are separable). Here we also see that points of the same class tend to spread around the class mean direction (after projecting-out the mean direction, point pairs tend to be orthogonal)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "git clone https://github.com/Irad-Zehavi/Few-Shot-Feature-Space.git\n",
    "cd Few-Shot-Feature-Space\n",
    "pip install .\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
